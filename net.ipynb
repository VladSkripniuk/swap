{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# connect TFRecords to graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_path = '/shared/TORCS_train.tfrecords'\n",
    "\n",
    "feature = {'train/fast': tf.FixedLenFeature([], tf.float32),\n",
    "    'train/dist_RR': tf.FixedLenFeature([], tf.float32),\n",
    "    'train/dist_MM': tf.FixedLenFeature([], tf.float32),\n",
    "    'train/dist_LL': tf.FixedLenFeature([], tf.float32),\n",
    "    'train/toMarking_RR': tf.FixedLenFeature([], tf.float32),\n",
    "    'train/toMarking_MR': tf.FixedLenFeature([], tf.float32),\n",
    "    'train/toMarking_ML': tf.FixedLenFeature([], tf.float32),\n",
    "    'train/toMarking_LL': tf.FixedLenFeature([], tf.float32),\n",
    "    'train/dist_R': tf.FixedLenFeature([], tf.float32),\n",
    "    'train/dist_L': tf.FixedLenFeature([], tf.float32),\n",
    "    'train/toMarking_R': tf.FixedLenFeature([], tf.float32),\n",
    "    'train/toMarking_M': tf.FixedLenFeature([], tf.float32),\n",
    "    'train/toMarking_L': tf.FixedLenFeature([], tf.float32),\n",
    "    'train/angle': tf.FixedLenFeature([], tf.float32),\n",
    "    'train/image': tf.FixedLenFeature([], tf.string)}\n",
    "\n",
    "# Create a list of filenames and pass it to a queue\n",
    "filename_queue = tf.train.string_input_producer([data_path], num_epochs=100)\n",
    "# Define a reader and read the next record\n",
    "reader = tf.TFRecordReader()\n",
    "_, serialized_example = reader.read(filename_queue)\n",
    "# Decode the record read by the reader\n",
    "features = tf.parse_single_example(serialized_example, features=feature)\n",
    "# Convert the image data from string back to the numbers\n",
    "image = tf.decode_raw(features['train/image'], tf.uint8)\n",
    "\n",
    "# Cast label data into float32\n",
    "fast = tf.cast(features['train/fast'], tf.float32)\n",
    "\n",
    "# rescale all targets to [0,1], numbers were taken from source code of DeepDriving\n",
    "dist_RR = tf.cast(features['train/dist_RR'], tf.float32) / 95.0 + 0.12\n",
    "dist_MM = tf.cast(features['train/dist_MM'], tf.float32) / 95.0 + 0.12\n",
    "dist_LL = tf.cast(features['train/dist_LL'], tf.float32) / 95.0 + 0.12\n",
    "\n",
    "toMarking_RR = tf.cast(features['train/toMarking_RR'], tf.float32) / 6.8752 - 0.48181\n",
    "toMarking_MR = tf.cast(features['train/toMarking_MR'], tf.float32) / 6.25 + 0.02\n",
    "toMarking_ML = tf.cast(features['train/toMarking_ML'], tf.float32) / 6.25 + 0.98\n",
    "toMarking_LL = tf.cast(features['train/toMarking_LL'], tf.float32) / 6.8752 + 1.48181\n",
    "\n",
    "dist_R = tf.cast(features['train/dist_R'], tf.float32) / 95.0 + 0.12\n",
    "dist_L = tf.cast(features['train/dist_L'], tf.float32) / 95.0 + 0.12\n",
    "\n",
    "toMarking_R = tf.cast(features['train/toMarking_R'], tf.float32) / 5.6249 - 0.34445\n",
    "toMarking_M = tf.cast(features['train/toMarking_M'], tf.float32) / 6.8752 + 0.39091\n",
    "toMarking_L = tf.cast(features['train/toMarking_L'], tf.float32) / 5.6249 + 1.34445\n",
    "\n",
    "angle = tf.cast(features['train/angle'], tf.float32) / 1.1 + 0.5\n",
    "\n",
    "# Reshape image data into the original shape\n",
    "image = tf.reshape(image, [210, 280, 3])\n",
    "\n",
    "# Any preprocessing here ...\n",
    "image = tf.cast(image, tf.float32)\n",
    "image = (image - 128.0) / 128.0\n",
    "\n",
    "# Creates batches by randomly shuffling tensors\n",
    "#images=1\n",
    "images, fasts, dist_RRs, dist_MMs, dist_LLs, toMarking_RRs, toMarking_MRs, toMarking_MLs, toMarking_LLs, dist_Rs, dist_Ls, toMarking_Rs, toMarking_Ms, toMarking_Ls, angles = tf.train.shuffle_batch(\n",
    "                    [image, fast, dist_RR, dist_MM, dist_LL, toMarking_RR, toMarking_MR, toMarking_ML,\n",
    "                        toMarking_LL, dist_R, dist_L, toMarking_R, toMarking_M, toMarking_L, angle],\n",
    "                        batch_size=64, capacity=50000, num_threads=8, min_after_dequeue=10000, allow_smaller_final_batch=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_true = tf.concat([tf.expand_dims(fasts,1), tf.expand_dims(dist_RRs,1), tf.expand_dims(dist_MMs,1), tf.expand_dims(dist_LLs,1), \n",
    "                    tf.expand_dims(toMarking_RRs,1), tf.expand_dims(toMarking_MRs,1), tf.expand_dims(toMarking_MLs,1),\n",
    "                    tf.expand_dims(toMarking_LLs,1), tf.expand_dims(dist_Rs,1), tf.expand_dims(dist_Ls,1), tf.expand_dims(toMarking_Rs,1),\n",
    "                    tf.expand_dims(toMarking_Ms,1), tf.expand_dims(toMarking_Ls,1), tf.expand_dims(angles,1)], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alexnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'shuffle_batch_1:0' shape=(64, 210, 280, 3) dtype=float32>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from alexnet import alexnet_v2\n",
    "\n",
    "\n",
    "logits, endpoints = alexnet_v2(images,\n",
    "               num_classes=1000,\n",
    "               is_training=True,\n",
    "               dropout_keep_prob=0.999,\n",
    "               spatial_squeeze=True,\n",
    "               scope='alexnet_v2')\n",
    "\n",
    "y_pred = endpoints['alexnet_v2/fc8']\n",
    "loss = tf.reduce_mean(tf.squared_difference(y_pred, y_true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('alexnet_v2/conv1',\n",
       "              <tf.Tensor 'alexnet_v2/conv1/Relu:0' shape=(64, 50, 68, 64) dtype=float32>),\n",
       "             ('alexnet_v2/pool1',\n",
       "              <tf.Tensor 'alexnet_v2/pool1/MaxPool:0' shape=(64, 24, 33, 64) dtype=float32>),\n",
       "             ('alexnet_v2/conv2',\n",
       "              <tf.Tensor 'alexnet_v2/conv2/Relu:0' shape=(64, 24, 33, 192) dtype=float32>),\n",
       "             ('alexnet_v2/pool2',\n",
       "              <tf.Tensor 'alexnet_v2/pool2/MaxPool:0' shape=(64, 11, 16, 192) dtype=float32>),\n",
       "             ('alexnet_v2/conv3',\n",
       "              <tf.Tensor 'alexnet_v2/conv3/Relu:0' shape=(64, 11, 16, 384) dtype=float32>),\n",
       "             ('alexnet_v2/conv4',\n",
       "              <tf.Tensor 'alexnet_v2/conv4/Relu:0' shape=(64, 11, 16, 384) dtype=float32>),\n",
       "             ('alexnet_v2/conv5',\n",
       "              <tf.Tensor 'alexnet_v2/conv5/Relu:0' shape=(64, 11, 16, 256) dtype=float32>),\n",
       "             ('alexnet_v2/pool5',\n",
       "              <tf.Tensor 'alexnet_v2/pool5/MaxPool:0' shape=(64, 5, 7, 256) dtype=float32>),\n",
       "             ('alexnet_v2/fc6',\n",
       "              <tf.Tensor 'alexnet_v2/fc6/Relu:0' shape=(64, 1, 1, 4096) dtype=float32>),\n",
       "             ('alexnet_v2/fc7',\n",
       "              <tf.Tensor 'alexnet_v2/fc7/Relu:0' shape=(64, 1, 1, 4096) dtype=float32>),\n",
       "             ('alexnet_v2/fc8',\n",
       "              <tf.Tensor 'alexnet_v2/fc8/squeezed:0' shape=(64, 14) dtype=float32>)])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "endpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mobilenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from mobilenet_v1 import mobilenet_v1\n",
    "\n",
    "logits, endpoints = mobilenet_v1(images, #tf.cast(images, tf.float32),\n",
    "                 num_classes=1000,\n",
    "                 dropout_keep_prob=0.999,\n",
    "                 is_training=True,\n",
    "                 min_depth=8,\n",
    "                 depth_multiplier=0.75,\n",
    "                 conv_defs=None,\n",
    "                 prediction_fn=tf.contrib.layers.softmax,\n",
    "                 spatial_squeeze=True,\n",
    "                 reuse=None,\n",
    "                 scope='MobilenetV1')\n",
    "\n",
    "y_pred = endpoints['Logits']\n",
    "loss = tf.reduce_mean(tf.squared_difference(y_pred, y_true))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:model_alex/19999.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "INFO:tensorflow:model_alex/39999.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "INFO:tensorflow:model_alex/59999.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "INFO:tensorflow:model_alex/79999.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "INFO:tensorflow:model_alex/99999.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "INFO:tensorflow:model_alex/119999.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
      "INFO:tensorflow:model_alex/139999.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "\n",
    "start_t = time()\n",
    "\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(loss)\n",
    "\n",
    "saver = tf.train.Saver(max_to_keep=None)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    # Initialize all global and local variables\n",
    "    init_op = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())\n",
    "    sess.run(init_op)\n",
    "\n",
    "    # Create a coordinator and run all QueueRunner objects\n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(coord=coord)\n",
    "\n",
    "    loss_list = []\n",
    "    for batch_index in range(140000):\n",
    "        loss_val, _ = sess.run([loss, train_step])\n",
    "        loss_list.append((loss_val, time() - start_t))\n",
    "        if batch_index % 10 == 0:\n",
    "            np.save('loss.npy', loss_list)\n",
    "        if (batch_index+1) % 20000 == 0:\n",
    "            saver.save(sess, \"model_alex/{}.ckpt\".format(batch_index))\n",
    "    \n",
    "    # Stop the threads\n",
    "    coord.request_stop()\n",
    "\n",
    "    # Wait for threads to stop\n",
    "    coord.join(threads)\n",
    "    sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Error reported to Coordinator: <class 'tensorflow.python.framework.errors_impl.CancelledError'>, Run call was cancelled\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'labels' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m----------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-3e26a9544287>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlbl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0;32mprint\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'labels' is not defined"
     ]
    }
   ],
   "source": [
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(loss)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    # Initialize all global and local variables\n",
    "    init_op = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())\n",
    "    sess.run(init_op)\n",
    "\n",
    "    # Create a coordinator and run all QueueRunner objects\n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(coord=coord)\n",
    "\n",
    "    for batch_index in range(5):\n",
    "        img, lbl = sess.run([images, labels])\n",
    "        print img.max(), img.min()\n",
    "        break\n",
    "        img = img.astype(np.uint8)\n",
    "        for j in range(6):\n",
    "            plt.subplot(2, 3, j+1)\n",
    "            plt.imshow(img[j, ...])\n",
    "            plt.title(str(lbl[j]))\n",
    "        plt.show()\n",
    "    # Stop the threads\n",
    "    coord.request_stop()\n",
    "\n",
    "    # Wait for threads to stop\n",
    "    coord.join(threads)\n",
    "    sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mobilenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hit:1 http://archive.ubuntu.com/ubuntu xenial InRelease\n",
      "Get:2 http://archive.ubuntu.com/ubuntu xenial-updates InRelease [102 kB]       \n",
      "Get:3 http://security.ubuntu.com/ubuntu xenial-security InRelease [102 kB]     \n",
      "Get:4 http://archive.ubuntu.com/ubuntu xenial-backports InRelease [102 kB]     \n",
      "Fetched 306 kB in 0s (448 kB/s)                                                \n",
      "Reading package lists... Done\n",
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "wget is already the newest version (1.17.1-1ubuntu1.2).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 27 not upgraded.\n"
     ]
    }
   ],
   "source": [
    "! apt-get update; apt-get install wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rm /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/slim/python/slim/nets/mobilenet_v1.py*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2017-07-27 14:18:09--  https://raw.githubusercontent.com/tensorflow/models/master/slim/nets/mobilenet_v1.py\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 19467 (19K) [text/plain]\n",
      "Saving to: 'mobilenet_v1.py.1'\n",
      "\n",
      "mobilenet_v1.py.1   100%[===================>]  19.01K  --.-KB/s    in 0.03s   \n",
      "\n",
      "2017-07-27 14:18:09 (668 KB/s) - 'mobilenet_v1.py.1' saved [19467/19467]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! wget https://raw.githubusercontent.com/tensorflow/models/master/slim/nets/mobilenet_v1.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from mobilenet_v1 import mobilenet_v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'shuffle_batch:0' shape=(10, 210, 280, 3) dtype=uint8>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logits, endpoints = mobilenet_v1(tf.cast(images, tf.float32),\n",
    "                 num_classes=1000,\n",
    "                 dropout_keep_prob=0.999,\n",
    "                 is_training=True,\n",
    "                 min_depth=8,\n",
    "                 depth_multiplier=1.0,\n",
    "                 conv_defs=None,\n",
    "                 prediction_fn=tf.contrib.layers.softmax,\n",
    "                 spatial_squeeze=True,\n",
    "                 reuse=None,\n",
    "                 scope='MobilenetV1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'AvgPool_1a': <tf.Tensor 'MobilenetV1/Logits/AvgPool_1a/AvgPool:0' shape=(10, 1, 1, 1024) dtype=float32>,\n",
       " 'Conv2d_0': <tf.Tensor 'MobilenetV1/MobilenetV1/Conv2d_0/Relu:0' shape=(10, 105, 140, 32) dtype=float32>,\n",
       " 'Conv2d_10_depthwise': <tf.Tensor 'MobilenetV1/MobilenetV1/Conv2d_10_depthwise/Relu:0' shape=(10, 14, 18, 512) dtype=float32>,\n",
       " 'Conv2d_10_pointwise': <tf.Tensor 'MobilenetV1/MobilenetV1/Conv2d_10_pointwise/Relu:0' shape=(10, 14, 18, 512) dtype=float32>,\n",
       " 'Conv2d_11_depthwise': <tf.Tensor 'MobilenetV1/MobilenetV1/Conv2d_11_depthwise/Relu:0' shape=(10, 14, 18, 512) dtype=float32>,\n",
       " 'Conv2d_11_pointwise': <tf.Tensor 'MobilenetV1/MobilenetV1/Conv2d_11_pointwise/Relu:0' shape=(10, 14, 18, 512) dtype=float32>,\n",
       " 'Conv2d_12_depthwise': <tf.Tensor 'MobilenetV1/MobilenetV1/Conv2d_12_depthwise/Relu:0' shape=(10, 7, 9, 512) dtype=float32>,\n",
       " 'Conv2d_12_pointwise': <tf.Tensor 'MobilenetV1/MobilenetV1/Conv2d_12_pointwise/Relu:0' shape=(10, 7, 9, 1024) dtype=float32>,\n",
       " 'Conv2d_13_depthwise': <tf.Tensor 'MobilenetV1/MobilenetV1/Conv2d_13_depthwise/Relu:0' shape=(10, 7, 9, 1024) dtype=float32>,\n",
       " 'Conv2d_13_pointwise': <tf.Tensor 'MobilenetV1/MobilenetV1/Conv2d_13_pointwise/Relu:0' shape=(10, 7, 9, 1024) dtype=float32>,\n",
       " 'Conv2d_1_depthwise': <tf.Tensor 'MobilenetV1/MobilenetV1/Conv2d_1_depthwise/Relu:0' shape=(10, 105, 140, 32) dtype=float32>,\n",
       " 'Conv2d_1_pointwise': <tf.Tensor 'MobilenetV1/MobilenetV1/Conv2d_1_pointwise/Relu:0' shape=(10, 105, 140, 64) dtype=float32>,\n",
       " 'Conv2d_2_depthwise': <tf.Tensor 'MobilenetV1/MobilenetV1/Conv2d_2_depthwise/Relu:0' shape=(10, 53, 70, 64) dtype=float32>,\n",
       " 'Conv2d_2_pointwise': <tf.Tensor 'MobilenetV1/MobilenetV1/Conv2d_2_pointwise/Relu:0' shape=(10, 53, 70, 128) dtype=float32>,\n",
       " 'Conv2d_3_depthwise': <tf.Tensor 'MobilenetV1/MobilenetV1/Conv2d_3_depthwise/Relu:0' shape=(10, 53, 70, 128) dtype=float32>,\n",
       " 'Conv2d_3_pointwise': <tf.Tensor 'MobilenetV1/MobilenetV1/Conv2d_3_pointwise/Relu:0' shape=(10, 53, 70, 128) dtype=float32>,\n",
       " 'Conv2d_4_depthwise': <tf.Tensor 'MobilenetV1/MobilenetV1/Conv2d_4_depthwise/Relu:0' shape=(10, 27, 35, 128) dtype=float32>,\n",
       " 'Conv2d_4_pointwise': <tf.Tensor 'MobilenetV1/MobilenetV1/Conv2d_4_pointwise/Relu:0' shape=(10, 27, 35, 256) dtype=float32>,\n",
       " 'Conv2d_5_depthwise': <tf.Tensor 'MobilenetV1/MobilenetV1/Conv2d_5_depthwise/Relu:0' shape=(10, 27, 35, 256) dtype=float32>,\n",
       " 'Conv2d_5_pointwise': <tf.Tensor 'MobilenetV1/MobilenetV1/Conv2d_5_pointwise/Relu:0' shape=(10, 27, 35, 256) dtype=float32>,\n",
       " 'Conv2d_6_depthwise': <tf.Tensor 'MobilenetV1/MobilenetV1/Conv2d_6_depthwise/Relu:0' shape=(10, 14, 18, 256) dtype=float32>,\n",
       " 'Conv2d_6_pointwise': <tf.Tensor 'MobilenetV1/MobilenetV1/Conv2d_6_pointwise/Relu:0' shape=(10, 14, 18, 512) dtype=float32>,\n",
       " 'Conv2d_7_depthwise': <tf.Tensor 'MobilenetV1/MobilenetV1/Conv2d_7_depthwise/Relu:0' shape=(10, 14, 18, 512) dtype=float32>,\n",
       " 'Conv2d_7_pointwise': <tf.Tensor 'MobilenetV1/MobilenetV1/Conv2d_7_pointwise/Relu:0' shape=(10, 14, 18, 512) dtype=float32>,\n",
       " 'Conv2d_8_depthwise': <tf.Tensor 'MobilenetV1/MobilenetV1/Conv2d_8_depthwise/Relu:0' shape=(10, 14, 18, 512) dtype=float32>,\n",
       " 'Conv2d_8_pointwise': <tf.Tensor 'MobilenetV1/MobilenetV1/Conv2d_8_pointwise/Relu:0' shape=(10, 14, 18, 512) dtype=float32>,\n",
       " 'Conv2d_9_depthwise': <tf.Tensor 'MobilenetV1/MobilenetV1/Conv2d_9_depthwise/Relu:0' shape=(10, 14, 18, 512) dtype=float32>,\n",
       " 'Conv2d_9_pointwise': <tf.Tensor 'MobilenetV1/MobilenetV1/Conv2d_9_pointwise/Relu:0' shape=(10, 14, 18, 512) dtype=float32>,\n",
       " 'Logits': <tf.Tensor 'MobilenetV1/Logits/Conv2d_1c_1x1/BiasAdd:0' shape=(10, 1, 1, 14) dtype=float32>,\n",
       " 'Predictions': <tf.Tensor 'MobilenetV1/Predictions/Reshape_1:0' shape=(10, 1, 1, 14) dtype=float32>}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = endpoints['Logits']\n",
    "y_true = tf.placeholder(shape=[None, 14], dtype=tf.float32)\n",
    "loss = tf.reduce_mean(tf.squared_difference(y_pred, y_true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No op named SSTableReaderV2 in defined operations.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-7ee69ca6930c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#First let's load meta graph and restore weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0msaver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_meta_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mobilenet/mobilenet_v1_0.50_128.ckpt.meta'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.pyc\u001b[0m in \u001b[0;36mimport_meta_graph\u001b[0;34m(meta_graph_or_file, clear_devices, import_scope, **kwargs)\u001b[0m\n\u001b[1;32m   1684\u001b[0m                                       \u001b[0mclear_devices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclear_devices\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1685\u001b[0m                                       \u001b[0mimport_scope\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimport_scope\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1686\u001b[0;31m                                       **kwargs)\n\u001b[0m\u001b[1;32m   1687\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mmeta_graph_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHasField\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"saver_def\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1688\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mSaver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msaver_def\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmeta_graph_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaver_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimport_scope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/meta_graph.pyc\u001b[0m in \u001b[0;36mimport_scoped_meta_graph\u001b[0;34m(meta_graph_or_file, clear_devices, graph, import_scope, input_map, unbound_inputs_col_name, restore_collections_predicate)\u001b[0m\n\u001b[1;32m    502\u001b[0m     importer.import_graph_def(\n\u001b[1;32m    503\u001b[0m         \u001b[0minput_graph_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimport_scope\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_map\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 504\u001b[0;31m         producer_op_list=producer_op_list)\n\u001b[0m\u001b[1;32m    505\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m     scope_to_prepend_to_names = \"/\".join(\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/importer.pyc\u001b[0m in \u001b[0;36mimport_graph_def\u001b[0;34m(graph_def, input_map, return_elements, name, op_dict, producer_op_list)\u001b[0m\n\u001b[1;32m    281\u001b[0m       \u001b[0;31m# Set any default attr values that aren't present.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mop\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mop_dict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 283\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'No op named %s in defined operations.'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    284\u001b[0m       \u001b[0mop_def\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mattr_def\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mop_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: No op named SSTableReaderV2 in defined operations."
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "sess=tf.Session()\n",
    "#First let's load meta graph and restore weights\n",
    "saver = tf.train.import_meta_graph('mobilenet/mobilenet_v1_0.50_128.ckpt.meta')\n",
    "saver.restore(sess,tf.train.latest_checkpoint('./'))\n",
    "\n",
    "\n",
    "# Access saved Variables directly\n",
    "# print(sess.run('bias:0'))\n",
    "# This will print 2, which is the value of bias that we saved\n",
    "\n",
    "\n",
    "# Now, let's access and create placeholders variables and\n",
    "# create feed-dict to feed new data\n",
    "\n",
    "graph = tf.get_default_graph()\n",
    "#w1 = graph.get_tensor_by_name(\"w1:0\")\n",
    "#w2 = graph.get_tensor_by_name(\"w2:0\")\n",
    "#feed_dict ={w1:13.0,w2:17.0}\n",
    "\n",
    "#Now, access the op that you want to run. \n",
    "#op_to_restore = graph.get_tensor_by_name(\"op_to_restore:0\")\n",
    "\n",
    "#print sess.run(op_to_restore,feed_dict)\n",
    "#This will print 60 which is calculated "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
